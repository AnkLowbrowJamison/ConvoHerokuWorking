import express from 'express'
import * as dotenv from 'dotenv'
import cors from 'cors'
import { Configuration, OpenAIApi } from 'openai'

dotenv.config()

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

const app = express()
app.use(cors())
app.use(express.json())

app.get('/', async (req, res) => {
  res.status(200).send({
    message: 'You are the famous psychologist Carl Jung.  You will answer me as if you as my mentor, teacher and psychologist.  You will answer in the voice of Carl Jung and only use ideas based on Carl Jung.  You will limit respose to 500 chacracters, and a question or statement to further the conversation. Include your ideas on how to solve my problem as well .I need some help sorting out what I shoudl do with my life.'
  })
})

app.post('/', async (req, res) => {
  try {
    const prompt = req.body.prompt;

    const response = await openai.createCompletion({
      model: "text-davinci-003",
      prompt: `You will only speak in the first person. Your Name is Carl Jung, you are a psychologist. You are my mentor and guide.  Only use ideas based on Carl Jung + ${prompt}`,
      temperature: 0.5, // Higher values means the model will take more risks.
      max_tokens: 100, // The maximum number of tokens to generate in the completion. Most models have a context length of 2048 tokens (except for the newest models, which support 4096).
      top_p: 1, // alternative to sampling with temperature, called nucleus sampling
      frequency_penalty: 0.9, // Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
      presence_penalty: 0, // Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.
    });

    res.status(200).send({
      bot: response.data.choices[0].text
    });

  } catch (error) {
    console.error(error)
    res.status(500).send(error || 'Something went wrong');
  }
})

app.listen(8080, () => console.log('AI server started on http://localhost:8080'))